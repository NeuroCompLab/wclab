[
  {
    "objectID": "research/publications.html",
    "href": "research/publications.html",
    "title": "Publications",
    "section": "",
    "text": "Preprints\nJ. David Wong-Campos, Dalia P. Ornelas-Huerta, Mackenzie Dion. Two-step excitation of fluorescent proteins with real intermediary states. arXiv, 2025.\nJ. David Wong-Campos, Mackenzie Dion. Multiphoton fluorescence excitation with real intermediary states. arXiv, 2025.\nDaniel G. Itkis, F. Phil Brooks III, Hunter C. Davis, Raphael Hotter, J. David Wong-Campos, Yitong Qi, Bill Z. Jia, Madeleine Howell, Marley Xiong, Rebecca Frank Hayward, Adam E. Cohen. Luminos: open-source software for bidirectional microscopy. bioRxiv, 2025.\nJ. David Wong-Campos, Pojeong Park, Hunter C. Davis, Yitong Qi, He Tian, Daniel G. Itkis, Doyeon Kim, Jonathan B. Grimm, Sarah E. Plutkis, Luke D. Lavis, Adam E. Cohen. Voltage dynamics of dendritic integration and back-propagation in vivo. bioRxiv, 2023.\n\n\n2026\nJ. David Wong-Campos‚Ä†, Pojeong Park‚Ä†, Byung Hun Lee‚Ä†, Hunter C. Davis, Yitong Qi, He Tian, Daniel G. Itkis, Doyeon Kim, Jonathan B. Grimm, Sarah E. Plutkis, Luke D. Lavis, Adam E. Cohen. Voltage dynamics of dendritic integration and back-propagation in vivo. In press, 2026.\n\n\n2025\nDoyeon Kim, Pojeong Park, Xiuyuan Li, J. David Wong-Campos, He Tian, Eric M. Moult, Jonathan B. Grimm, Luke D. Lavis, Adam E. Cohen. EPSILON: a method for pulse-chase labeling to probe synaptic AMPAR exocytosis during memory formation. Nature Neuroscience, 2025.\nPojeong Park‚Ä†, J. David Wong-Campos‚Ä†, Daniel G. Itkis, Byung Hun Lee, Yitong Qi, Hunter C. Davis, Benjamin Antin, Amol Pasarkar, Jonathan B. Grimm, Sarah E. Plutkis, Luke D. Lavis, Adam E. Cohen. Dendritic excitations govern back-propagation via a spike-rate accelerometer. Nature Communications, 2025.\n\n\n2024\nF. Phil Brooks III, Hunter C. Davis, J. David Wong-Campos, Adam E. Cohen. Optical constraints on two-photon voltage imaging. Neurophotonics, 2024.\n\n\n2023\nBill Z. Jia, Yitong Qi, J. David Wong-Campos, Sean G. Megason, Adam E. Cohen. A bioelectrical phase transition patterns the first vertebrate heartbeats. Nature, 2023.\nHe Tian, Hunter C. Davis, J. David Wong-Campos, Pojeong Park, Linlin Z. Fan, Benjamin Gmeiner, Shahinoor Begum, Christopher A. Werley, Gabriel B. Borja, Hansini Upadhyay, Himali Shah, Jane Jacques, Yitong Qi, Vicente Parot, Karl Deisseroth, Adam E. Cohen. Video-based pooled screening yields improved far-red genetically encoded voltage indicators. Nature Methods, 2023.\n\n\n2022\nAndrew T. Landau, Pojeong Park, J. David Wong-Campos, He Tian, Adam E. Cohen, Bernardo L. Sabatini. Dendritic branch structure compartmentalizes voltage-dependent calcium influx in cortical layer 2/3 pyramidal cells. eLife, 2022.\n\n\n2021\nJ. David Wong-Campos, J. V. Porto, Adam E. Cohen. Which way does stimulated emission go?. Journal of Physical Chemistry A, 2021.\n\n\n2020\nYunseong Nam, Jwo-Sy Chen, Neal C. Pisenti, Kenneth Wright, Conor Delaney, Dmitri Maslov, Kenneth R. Brown, Stewart Allen, Jason M. Amini, Joel Apisdorf, Kristin M. Beck, Aleksey Blinov, Vandiver Chaplin, Mika Chmielewski, Coleman Collins, Shantanu Debnath, Andrew M. Ducore, Kai M. Hudek, Matthew Keesan, Sarah M. Kreikemeier, Jonathan Mizrahi, Phil Solomon, Mike Williams, J. David Wong-Campos, Christopher Monroe, Jungsang Kim. Ground-state energy estimation of the water molecule on a trapped-ion quantum computer. npj Quantum Information, 2020.\nJuan F. Yee-de Le√≥n, Brenda Soto-Garc√≠a, Diana Ar√°iz-Hern√°ndez, Jes√∫s Rolando Delgado-Balderas, Miguel Esparza, Carlos Aguilar-Avelar, J. David Wong-Campos, Franco Chac√≥n, Jos√© Y. L√≥pez-Hern√°ndez, A. Mauricio Gonz√°lez-Trevi√±o, Jos√© R. Yee-de Le√≥n, Jorge L. Zamora-Mendoza, Mario M. Alvarez, Grissel Trujillo-de Santiago, Lauro S. G√≥mez-Guerra, Celia N. S√°nchez-Dom√≠nguez, Liza P. Velarde-Calvillo, Alejandro Abarca-Blanco. Characterization of a novel automated microfiltration device for the efficient isolation and analysis of circulating tumor cells from clinical blood samples. Scientific Reports, 2020.\n\n\n2019\nCarlos Aguilar-Avelar, Brenda Soto-Garc√≠a, Diana Ar√°iz-Hern√°ndez, Juan F. Yee-de Le√≥n, Miguel Esparza, Franco Chac√≥n, Jes√∫s Rolando Delgado-Balderas, Mario M. Alvarez, Grissel Trujillo-de Santiago, Lauro S. G√≥mez-Guerra, Liza P. Velarde-Calvillo, Alejandro Abarca-Blanco, J. David Wong-Campos. High-throughput automated microscopy of circulating tumor cells. Scientific Reports, 2019.\nKenneth Wright, Kristin M. Beck, Shantanu Debnath, Jason M. Amini, Yunseong Nam, Nade Grzesiak, Jwo-Sy Chen, Neal C. Pisenti, Mika Chmielewski, Coleman Collins, Kai M. Hudek, Jonathan Mizrahi, J. David Wong-Campos, Stewart Allen, Joel Apisdorf, Phil Solomon, Mike Williams, Andrew M. Ducore, Aleksey Blinov, Sarah M. Kreikemeier, Vandiver Chaplin, Matthew Keesan, Christopher Monroe, Jungsang Kim. Benchmarking an 11-qubit quantum computer. Nature Communications, 2019.\n\n\n2017\nJ. David Wong-Campos, Steven A. Moses, Kale G. Johnson, Christopher Monroe. Demonstration of two-atom entanglement with ultrafast optical pulses. Physical Review Letters, 2017.\nKale G. Johnson, J. David Wong-Campos, Brian Neyenhuis, Jonathan Mizrahi, Christopher Monroe. Ultrafast creation of large Schr√∂dinger cat states of an atom. Nature Communications, 2017.\n\n\n2016\nJ. David Wong-Campos, Kale G. Johnson, Brian Neyenhuis, Jonathan Mizrahi, Christopher Monroe. High-resolution adaptive imaging of a single atom. Nature Photonics, 2016.\nKale G. Johnson, J. David Wong-Campos, Alessandro Restelli, Kyle A. Landsman, Brian Neyenhuis, Jonathan Mizrahi, Christopher Monroe. Active stabilization of ion trap radiofrequency potentials. Review of Scientific Instruments, 2016.\n\n\n2015\nKale G. Johnson, Brian Neyenhuis, Jonathan Mizrahi, J. David Wong-Campos, Christopher Monroe. Sensing atomic motion from the zero point to room temperature with ultrafast atom interferometry. Physical Review Letters, 2015.\n\n\n2014\nJ. E. Hoffman, S. Ravets, J. A. Grover, P. Solano, P. R. Kordell, J. David Wong-Campos, L. A. Orozco, S. L. Rolston. Ultrahigh transmission optical nanofibers. AIP Advances, 2014.\n\n\n2013\nS. Ravets, J. E. Hoffman, P. R. Kordell, J. David Wong-Campos, S. L. Rolston, L. A. Orozco. Intermodal energy transfer in a tapered optical fiber: optimizing transmission. Journal of the Optical Society of America A, 2013.\n\n\n2012\nPedro A. Sanchez-Serrano, J. David Wong-Campos, Servando Lopez-Aguayo, Julio C. Guti√©rrez-Vega. Engineering of nondiffracting beams with genetic algorithms. Optics Letters, 2012.\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/cellpose3.html",
    "href": "research/posts/cellpose3.html",
    "title": "Cellpose3: one-click image restoration for improved cellular segmentation",
    "section": "",
    "text": "Generalist methods for cellular segmentation have good out-of-the-box performance on a variety of image types; however, existing methods struggle for images that are degraded by noise, blurring or undersampling, all of which are common in microscopy. We focused the development of Cellpose3 on addressing these cases and here we demonstrate substantial out-of-the-box gains in segmentation and image quality for noisy, blurry and undersampled images. Unlike previous approaches that train models to restore pixel values, we trained Cellpose3 to output images that are well segmented by a generalist segmentation model, while maintaining perceptual similarity to the target images. Furthermore, we trained the restoration models on a large, varied collection of datasets, thus ensuring good generalization to user images. We provide these tools as ‚Äòone-click‚Äô buttons inside the graphical interface of Cellpose as well as in the Cellpose API.\n\npaper | talk+tutorial | slides | code | news coverage | preprint\n\nThis is an upgrade to Cellpose; if you‚Äôre unfamiliar with Cellpose, check it out here.\nDenoising, deblurring and upsampling examples:\n\n\n\nCellpose3 slides:\n\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/highprecision.html",
    "href": "research/posts/highprecision.html",
    "title": "High precision coding in visual cortex",
    "section": "",
    "text": "Abstract\n\n\n\nIndividual neurons in visual cortex provide the brain with unreliable estimates of visual features. It is not known whether the single-neuron variability is correlated across large neural populations, thus impairing the global encoding of stimuli. We recorded simultaneously from up to 50,000 neurons in mouse primary visual cortex (V1) and in higher order visual areas and measured stimulus discrimination thresholds of 0.35¬∞ and 0.37¬∞, respectively, in an orientation decoding task. These neural thresholds were almost 100 times smaller than the behavioral discrimination thresholds reported in mice. This discrepancy could not be explained by stimulus properties or arousal states. Furthermore, behavioral variability during a sensory discrimination task could not be explained by neural variability in V1. Instead, behavior-related neural activity arose dynamically across a network of non-sensory brain areas. These results imply that perceptual discrimination in mice is limited by downstream decoders, not by neural noise in sensory representations.\n\n\npaper | talk | HHMI news article | Scientifica article | preprint | data | code | original tweeprint\n\n\n\n\n\n\nThread:\n\nSingle neurons in the brain can‚Äôt be depended on for reliable information. Here are some neurons from our recent study, recorded twice in response to the same visual stimuli. Different neurons are active at different times!\n\n\n\n\nAsk a neuron what angle the corner of your screen makes and it will say 75 degrees right now, 100 degrees in 5 minutes, and some other random number close to 90 every time you ask.\n\n\n\nThat is not how a computational device should work! Imagine if your calculator gave different answers every time\n\n\n\n\nThis makes our lives as neuroscientists hard. Single measurements of neurons are not reliable (gray dots), and we need to repeat the measurements many times to average out the noise (black line).\n\n\n\n\nMaybe, we thought, the brain uses some kind of averaging over its millions of noisy neurons to get a clean estimate of what it‚Äôs looking at.\n\n\n\nIf that was true, there would be ‚Äúmagical‚Äù combinations of neurons, which averaged would give just the right answer. Can we find these ‚Äúmagical‚Äù combinations by looking at the brain while it‚Äôs looking at our images? We used a microscope to record the activity of ~20,000 neurons simultaneously. Here is all of them from one session in random colors.\n\n\n\n\nWe used linear regression to find weights for each neuron that combine their activities into ‚Äúsuper-neurons‚Äù.\n\n\n\n\nThese super-neurons were much less noisy than single neurons. In fact, the super-neurons could tell the difference between 45 and 46 degrees on 95% of the test trials. Can you?\n\n\n\n\nImagine asking a mouse to distinguish such small differences‚Ä¶ Our colleagues in @BenucciLab actually tried! The mouse could only tell apart angle differences of 29 degrees, which was about 100 times worse than the neurons.\n\n\n\n\nEven for humans it‚Äôs difficult, but I bet you can see the difference if I make the pictures into a movie.\n\n\n\n\nWe conclude that mice have a lot of information in their brains, which are 1000x smaller than ours.\n\n\n\n\nThey can‚Äôt communicate this information to us, but that does not mean they don‚Äôt use it, for example as a first step to another computation.\n\n\n\n\nWe hope to find out in the future what these other computations might be. We publicly shared the data and code from this paper if anyone wants to dig further. data: (figshare) code: (github.com/MouseLand/stringer-et-al-2019)\n\n\nThe End.\n\n\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/minimodels.html",
    "href": "research/posts/minimodels.html",
    "title": "A simplified minimodel of visual cortical neurons",
    "section": "",
    "text": "Abstract\n\n\n\nArtificial neural networks (ANNs) have been shown to predict neural responses in primary visual cortex (V1) better than classical models. However, this performance often comes at the expense of simplicity and interpretability. Here we introduce a new class of simplified ANN models that can predict over 70% of the response variance of V1 neurons. To achieve this high performance, we first recorded a new dataset of over 29,000 neurons responding to up to 65,000 natural image presentations in mouse V1. We found that ANN models required only two convolutional layers for good performance, with a relatively small first layer. We further found that we could make the second layer small without loss of performance, by fitting individual ‚Äúminimodels‚Äù to each neuron. Similar simplifications applied for models of monkey V1 neurons. We show that the minimodels can be used to gain insight into how stimulus invariance arises in biological neurons.\n\n\npaper | code | tutorial | data | news coverage | preprint | original tweeprint\n\n\nThread by Fengtong Du:\n\nPredicting neural activity is notoriously difficult and requires complicated models. Here we develop simple ‚Äúminimodels‚Äù which explain 70% of neural variance in V1! üê≠üêí\n\n\n\n\n\nWe started with population-level models, fitting all neurons together with 4 shared conv layers. These models performed better than past models because we showed many more images. The model predicted monkey V1 responses well too.\n\n\n\nBut we didn‚Äôt need such a deep network: two convolutional layers were sufficient, in both mice and monkeys. Also, the first layer could be very small, 16 filters, while the second layer did need to be large, in line with the high dimensionality of V1.\n\n\n\n\nThis structure ‚Äì small first convolutional layer and large second convolutional layer ‚Äì was advantageous for performing visual tasks, such as texture classification and image recognition.\n\n\n\n\nNext, can we simplify the wide second layer further? We found that using more neurons to fit the model did NOT help! This suggested that we could fit smaller models to individual neurons.\n\n\n\n\nSo we built a minimodel for each neuron, matching the performance of the best models. On average, mouse minimodels had 32 conv2 filters and monkey minimodels had 7, much fewer than the 320 filters in our previous model.\n\n\n\n\nNow equipped with a minimodel for each neuron, we used them to understand how the visual invariance of a single neuron develops across the model stages. We designed a metric, fraction of category variance (FECV) to measure this invariance.\n\n\n\n\nWe found that instead of gradually increasing, the invariance primarily emerges at the readout stage and is influenced by both pooling size and input channel similarity.\n\n\n\n\nWith these minimodels, we can also visualize the high and low FECV neurons in mouse and monkey V1.\n\n\n\n\nIn summary, we found single-neuron minimodels are just as powerful as larger ones! It offers an accurate and interpretable approach to studying visual computation across different species and experimental contexts. üê≠üêí\n\nHuge thanks to Janelia! Thanks to the GENIE project, the Vivarium staff, Sarah Lindo and Sal DiLisio for surgery, Jon Arnold for designing headbars and coverslips, Dan Flickinger for microscopy, and Jon Arnold and Tobias Goulet for engineering support.\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/cellpose.html",
    "href": "research/posts/cellpose.html",
    "title": "Cellpose: a generalist algorithm for cellular segmentation",
    "section": "",
    "text": "Abstract\n\n\n\nMany biological applications require the segmentation of cell bodies, membranes and nuclei from microscopy images. Deep learning has enabled great progress on this problem, but current methods are specialized for images that have large training datasets. Here we introduce a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells from a wide range of image types and does not require model retraining or parameter adjustments. Cellpose was trained on a new dataset of highly varied images of cells, containing over 70,000 segmented objects. We also demonstrate a three-dimensional (3D) extension of Cellpose that reuses the two-dimensional (2D) model and does not require 3D-labeled data. To support community contributions to the training data, we developed software for manual labeling and for curation of the automated results. Periodically retraining the model on the community-contributed data will ensure that Cellpose improves constantly.\n\n\npaper | talk | news article | preprint | code | original tweeprint\n\n\nThread:\n\nReleasing Cellpose, a generalist algorithm for cellular segmentation. Try it now directly on the website www.cellpose.org, or install the GUI with pip install cellpose:\n\n\n\n\n\n\n\nWe developed Cellpose as a generalist algorithm, because many small and big problems in biology require cell segmentation, and there just isn‚Äôt enough time to write a new pipeline for every type of data.\n\n\n\n\n\n\nWe trained Cellpose on a diverse set of 608 images, collected and segmented by us. See the t-SNE plot of image styles below.\n\n\n\n\n\n\nOut-of-the box, Cellpose can segment a large variety of images from different types of microscopy, different tissues and different stains or fluorescent tags. It can even segment rocks, jellyfish and sea urchins.\n\n\n\n\n\n\nThe GUI lets you manually segment your own images at a speed of 300-600 objects per hour. Cellpose doesn‚Äôt need super precise outlines.\n\n\n\n\n\n\n\nSend us your manual segmentations and we‚Äôll include them in the next Cellpose release, making the model better for yourself and everyone else! P.S. thanks for sending us your segmentations, they are now in the ‚Äúcyto2‚Äù model!\n\n\n\n\n\n\nAlso you can perform 3D segmentation without 3D training data!\n\n\n\n\n\n\nCheck out the [paper] for much more: nucleus segmentation, comparisons to previous state-of-the-art methods, a cell size prediction network etc.\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/facemap.html",
    "href": "research/posts/facemap.html",
    "title": "Facemap: a framework for modeling neural activity based on orofacial tracking",
    "section": "",
    "text": "Recent studies in mice have shown that orofacial behaviors drive a large fraction of neural activity across the brain. To understand the nature and function of these signals, we need better computational models to characterize the behaviors and relate them to neural activity. Here we developed Facemap, a framework consisting of a keypoint tracker and a deep neural network encoder for predicting neural activity. Our algorithm for tracking mouse orofacial behaviors was more accurate than existing pose estimation tools, while the processing speed was several times faster, making it a powerful tool for real-time experimental interventions. The Facemap tracker was easy to adapt to data from new labs, requiring as few as 10 annotated frames for near-optimal performance. We used the keypoints as inputs to a deep neural network which predicts the activity of ~50,000 simultaneously-recorded neurons and, in visual cortex, we doubled the amount of explained variance compared to previous methods. Using this model, we found that the neuronal activity clusters that were well predicted from behavior were more spatially spread out across cortex. We also found that the deep behavioral features from the model had stereotypical, sequential dynamics that were not reversible in time. In summary, Facemap provides a stepping stone toward understanding the function of the brain-wide neural signals and their relation to behavior.\n\narticle | preprint | code | Nature Methods highlight | news article | talk | tweeprint\n\n\n\n\nTutorial:\n\n\nGUI:\n \n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/rastermap.html",
    "href": "research/posts/rastermap.html",
    "title": "Rastermap: A Discovery Method for Neural Population Recordings",
    "section": "",
    "text": "Abstract\n\n\n\nNeurophysiology has long progressed through exploratory experiments and chance discoveries. Anecdotes abound of researchers listening to spikes in real time and noticing patterns of activity related to ongoing stimuli or behaviors. With the advent of large-scale recordings, such close observation of data has become difficult. To find patterns in large-scale neural data, we developed ‚ÄòRastermap‚Äô, a visualization method that displays neurons as a raster plot after sorting them along a one-dimensional axis based on their activity patterns. We benchmarked Rastermap on realistic simulations and then used it to explore recordings of tens of thousands of neurons from mouse cortex during spontaneous, stimulus-evoked and task-evoked epochs. We also applied Rastermap to whole-brain zebrafish recordings; to wide-field imaging data; to electrophysiological recordings in rat hippocampus, monkey frontal cortex and various cortical and subcortical regions in mice; and to artificial neural networks. Finally, we illustrate high-dimensional scenarios where Rastermap and similar algorithms cannot be used effectively.\n\n\npaper | code | talk | slides | news article | preprint | original tweeprint\n\n\nMake your next discovery using #Rastermap, a visualization method for large-scale neural data.\n\n\n\n\n\nYou can explore your data in the #Rastermap graphical user interface:\n\n\n\n\n\nRastermap finds single-trial sequences of neural activity in a virtual reality experiment:\n\n\n\n\n\nRastermap finds movement-related structure in spontaneous activity in complete darkness:\n\n\n\n\n\nRastermap sorting of hippocampus data from Grosmark & Buzsaki, 2016:\n\n\n\n\n\nRastermap sorting of wholebrain zebrafish activity from Chen et al, 2018:\n\n\n\n\n\nRastermap sorting of neurons from Reinforcement Learning agents playing Atari games:\n\n\n\n\n\nLearn more on our github: https://github.com/MouseLand/rastermap. Rastermap is fast thanks to numpy, scipy, numba, and scikit-learn. The GUI is powered by pyqt and pyqtgraph, and supports npz, npy, mat and nwb ophys files.\nExcited to see new datasets explored with this! If you have issues, please post an issue on the Rastermap github: https://github.com/MouseLand/rastermap/issues.\n\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/spontaneous.html",
    "href": "research/posts/spontaneous.html",
    "title": "Spontaneous behaviors drive multidimensional, brainwide activity",
    "section": "",
    "text": "Abstract\n\n\n\nNeuronal populations in sensory cortex produce variable responses to sensory stimuli and exhibit intricate spontaneous activity even without external sensory input. Cortical variability and spontaneous activity have been variously proposed to represent random noise, recall of prior experience, or encoding of ongoing behavioral and cognitive variables. Recording more than 10,000 neurons in mouse visual cortex, we observed that spontaneous activity reliably encoded a high-dimensional latent state, which was partially related to the mouse‚Äôs ongoing behavior and was represented not just in visual cortex but also across the forebrain. Sensory inputs did not interrupt this ongoing signal but added onto it a representation of external stimuli in orthogonal dimensions. Thus, visual cortical population activity, despite its apparently noisy structure, reliably encodes an orthogonal fusion of sensory and multidimensional behavioral information.\n\n\npaper | talk | Quanta news article | Simons news article | preprint | data | code | original tweeprint\n\n\n\n\nThread:\n\nNeurons in the brain are very chatty: they fire action potentials, their basic unit of communication, even when there is nothing to communicate. For example in this video, in visual cortex recordings in complete darkness. Lots of chatting:\n\n\n\n\n\nThe puzzle is that these neurons are in visual cortex, so they should only be talking about what the eyes tell them. But here they are in pitch black darkness, chatting continuously.\n\nThis has been known for a long time, ever since the first recordings of single neurons in live brains.\n\nNeuroscientists are divided. Some think the chattering is just noise. Others think the neurons are chatting about something very important, we just can‚Äôt understand their language.\n\nRecording one neuron at a time, we have no chance to understand this language. It‚Äôs like hearing only half of a conversation:\n\n\nNeuron B: ‚ÄúYou‚Äôre just paranoid.‚Äù Neuron B: ‚ÄúHe started it.‚Äù\n\n\nBut really more like only a millionth of it:\n\n\nNeuron B: ‚Äút‚Äù\n\nThe number of recorded neurons has improved dramatically. We can now pick up whole conversations:\n\n\nNeuron A: I think I saw something. Neuron B: You‚Äôre just paranoid. Neuron C: Shh, I‚Äôm trying to sleep here. Neuron B: He started it. Neuron D: I think we‚Äôre going to be eaten.\n\nWe recorded about 10,000 neurons and sorted their activity using an algorithm so you can see next to each other groups of neurons that talk about the same things:\n\n\n\n\nThis showed us there were many conversations going on at the same time. But we still didn‚Äôt know what the conversations were about.\n\nAnd for quite a while, we were clueless‚Ä¶\n\nBut then it hit us. It was movements! The neurons were talking about movements!\n\nThe neurons were following in real time the motor actions of the mouse, each of them chatting about a different type of movement!\n\nIn mice, these motor actions are things like running, whisking, grooming and sniffing:\n\n\n\n\n\nWe detected these motor actions from the videos using automated algorithms:\n\n\n\nAnd then used all this motor information to predict which neurons were chatting and when:\n\n\n\n\nWe predicted a lot, about 50% of the neural conversation. In fact, in every area of the brain we looked, we could predict about that much:\n\n\n\n\nTo conclude, neurons in mice, and probably in your brain, are always chatting about what you‚Äôre doing, even if these neurons normally respond to images or sounds or tickling.\n\nWhy are they having this conversation about motor actions? It‚Äôs a bit of a mystery. Our theory is that it helps the brain detect coincidences: when action X and stimulus Y happen at the same time.\n\nAction X + stimulus Y could often lead to a consequence Z. Z can be a juicy reward, or a dangerous situation.\n\nLuckily, there is a neuron in your brain somewhere who noticed the coincidence XY, so it can alert everyone about the possible consequence Z before it happens!\n\nThere are more esoteric interpretations, like the chattering being the mechanistic substrate of consciousness. We‚Äôll leave this for others to ponder.\n\nWe shared the data, and the code to run the analyses. data: (figshare) code: (github.com/MouseLand/stringer-pachitariu-et-al-2018a)\n\n\n\nP.S. We followed up on this story here!\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/kilosort.html",
    "href": "research/posts/kilosort.html",
    "title": "Spike sorting with Kilosort4",
    "section": "",
    "text": "Spike sorting is the computational process of extracting the firing times of single neurons from recordings of local electrical fields. This is an important but hard problem in neuroscience, made complicated by the nonstationarity of the recordings and the dense overlap in electrical fields between nearby neurons. To address the spike-sorting problem, we have been openly developing the Kilosort framework. Here we describe the various algorithmic steps introduced in different versions of Kilosort. We also report the development of Kilosort4, a version with substantially improved performance due to clustering algorithms inspired by graph-based approaches. To test the performance of Kilosort, we developed a realistic simulation framework that uses densely sampled electrical fields from real experiments to generate nonstationary spike waveforms and realistic noise. We found that nearly all versions of Kilosort outperformed other algorithms on a variety of simulated conditions and that Kilosort4 performed best in all cases, correctly identifying even neurons with low amplitudes and small spatial extents in high drift conditions.\n\npaper | code | talk | HHMI news article | preprint\n\n\n\n\nGraphical user interface:\n\n\n\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "positions.html",
    "href": "positions.html",
    "title": "Positions",
    "section": "",
    "text": "We are seeking postdocs to investigate how large populations of neurons perform complex computations. We collect and analyze recordings of 50,000+ neurons, developing machine learning tools to extract computational principles from these large-scale datasets. Please reach out to Carsen Stringer (stringerc@hhmi.org) with your CV if you are interested. We are looking for candidates with experience performing imaging experiments in animals and ideally some experience analyzing large-scale neural datasets. Please share a link to your github profile on your CV when applying, especially if you are coming from a computational background.\nAll research is internally funded by the Howard Hughes Medical Institute with highly competitive benefits. A first-year postdoc is compensated at a rate of $74,200.00 annually. For information about Janelia, please visit www.janelia.org/about-us. For more information about our neuroscience department, please visit www.janelia.org/our-research/mechanistic-cognitive-neuroscience. For more information about being at Janelia, please check out these videos. Janelia provides incredible support for scientific research (surgical support, animal behavior training, and optical engineering), enabling researchers to focus on their specific scientific questions.\nDiversity, equity and inclusion are important values at Janelia, and applicants should be dedicated to ensuring kindness and inclusion in their interactions with the scientific community and with other employees at Janelia."
  },
  {
    "objectID": "positions.html#postdocs",
    "href": "positions.html#postdocs",
    "title": "Positions",
    "section": "",
    "text": "We are seeking postdocs to investigate how large populations of neurons perform complex computations. We collect and analyze recordings of 50,000+ neurons, developing machine learning tools to extract computational principles from these large-scale datasets. Please reach out to Carsen Stringer (stringerc@hhmi.org) with your CV if you are interested. We are looking for candidates with experience performing imaging experiments in animals and ideally some experience analyzing large-scale neural datasets. Please share a link to your github profile on your CV when applying, especially if you are coming from a computational background.\nAll research is internally funded by the Howard Hughes Medical Institute with highly competitive benefits. A first-year postdoc is compensated at a rate of $74,200.00 annually. For information about Janelia, please visit www.janelia.org/about-us. For more information about our neuroscience department, please visit www.janelia.org/our-research/mechanistic-cognitive-neuroscience. For more information about being at Janelia, please check out these videos. Janelia provides incredible support for scientific research (surgical support, animal behavior training, and optical engineering), enabling researchers to focus on their specific scientific questions.\nDiversity, equity and inclusion are important values at Janelia, and applicants should be dedicated to ensuring kindness and inclusion in their interactions with the scientific community and with other employees at Janelia."
  },
  {
    "objectID": "team/dwc.html",
    "href": "team/dwc.html",
    "title": "David Wong-Campos",
    "section": "",
    "text": "David (Dahveed) Wong-Campos is a group leader at HHMI Janelia Research Campus. His lab is interested in understanding the fundamental limits of measurement in live biological samples.\nHere‚Äôs a¬†interview¬†of David.\nwongcamposd@hhmi.org  google scholar"
  },
  {
    "objectID": "team/dwc.html#education-and-experience",
    "href": "team/dwc.html#education-and-experience",
    "title": "David Wong-Campos",
    "section": "Education and Experience",
    "text": "Education and Experience\nTriplet Imaging, USA | 2025 Founder\nHarvard University, Cambridge, USA | 2024 Postdoctoral fellow  Advisor: Adam Cohen\nIonQ, College Park, USA | 2020  Senior Physicist \nUniversity of Maryland, College Park, USA | 2018 PhD in Physics  Advisor: Chris Monroe\nInstituto Tecnol√≥gico y de Estudios Superiores de Monterrey, Campus Monterrey (ITESM), Mexico | 2012  BSc Engineering Physics"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "notes.html#notes",
    "href": "notes.html#notes",
    "title": "Notes",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "team/index.html",
    "href": "team/index.html",
    "title": "Team",
    "section": "",
    "text": "David Wong-Campos  Group Leader\n\n\n    Dalia Ornelas-Huerta  Research Specialist\n\n\n    Jose Zepeda  Postdoc\n\n\n\n\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Observing Life in Motion",
    "section": "",
    "text": "Biological systems compute through fast, distributed, and adaptive dynamics that are difficult to measure with existing tools. Our research focuses on developing quantitative optical instrumentation to measure biological activity with high temporal precision, stability, and interpretability, and on using these measurements to understand how cells‚Äîand particularly neurons‚Äîprocess information during behavior and learning."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Observing Life in Motion",
    "section": "News",
    "text": "News\nJanuary 5, 2026: Patch clamp extraordinarie Jose Zepeda joins. Welcome Jose!\nMarch 10, 2025: The Biological Dynamics & Instrumentation Group is created! Instrumentation whiz Dalia joins as the first teammate. Welcome Dalia!"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Observing Life in Motion",
    "section": "Resources",
    "text": "Resources\n\nAbout HHMI Janelia Research Campus\nHHMI‚Äôs values\nBeing a postdoc at Janelia"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "We address fundamental limitations of current optical techniques for recording the activity and dynamics of biological samples. Below are three directions that attack neural recordings throughput (1), absolute quantification for stable recordings of microscopy data (2), and exploration of novel photophysical schemes for new imaging methods (3).\n\nSystems-Level Inference\nQuantitative Optical Measurements\nPhotophysics of Dark States"
  },
  {
    "objectID": "research/index.html#research-areas",
    "href": "research/index.html#research-areas",
    "title": "Research",
    "section": "",
    "text": "We address fundamental limitations of current optical techniques for recording the activity and dynamics of biological samples. Below are three directions that attack neural recordings throughput (1), absolute quantification for stable recordings of microscopy data (2), and exploration of novel photophysical schemes for new imaging methods (3).\n\nSystems-Level Inference\nQuantitative Optical Measurements\nPhotophysics of Dark States"
  },
  {
    "objectID": "research/posts/inhibitory.html",
    "href": "research/posts/inhibitory.html",
    "title": "Inhibitory control of correlated intrinsic variability in cortical networks",
    "section": "",
    "text": "Abstract\n\n\n\nCortical networks exhibit intrinsic dynamics that drive coordinated, large-scale fluctuations across neuronal populations and create noise correlations that impact sensory coding. To investigate the network-level mechanisms that underlie these dynamics, we developed novel computational techniques to fit a deterministic spiking network model directly to multi-neuron recordings from different rodent species, sensory modalities, and behavioral states. The model generated correlated variability without external noise and accurately reproduced the diverse activity patterns in our recordings. Analysis of the model parameters suggested that differences in noise correlations across recordings were due primarily to differences in the strength of feedback inhibition. Further analysis of our recordings confirmed that putative inhibitory neurons were indeed more active during desynchronized cortical states with weak noise correlations. Our results demonstrate that network models with intrinsically-generated variability can accurately reproduce the activity patterns observed in multi-neuron recordings and suggest that inhibition modulates the interactions between intrinsic dynamics and sensory inputs to control the strength of noise correlations.\n\n\npaper\n\n\nOur brains contain billions of neurons, which are continually producing electrical signals to relay information around the brain. Yet most of our knowledge of how the brain works comes from studying the activity of one neuron at a time. Recently, studies of multiple neurons have shown that they tend to be active together in short bursts called ‚Äúup‚Äù states, which are followed by periods in which they are less active called ‚Äúdown‚Äù states. When we are sleeping or under a general anesthetic, the neurons may be completely silent during down states, but when we are awake the difference in activity between the two states is usually less extreme. However, it is still not clear how the neurons generate these patterns of activity.\n\nTo address this question, we studied the activity of neurons in the brains of awake and anesthetized rats, mice and gerbils. The experiments recorded electrical activity from many neurons at the same time and found a wide range of different activity patterns:\n\n\n\n\n\n\nWe built a spiking neuronal network model which recapitulated the up and down states in the data:\n\n\n\n\n\n\nWe fit the model parameters directly to the neural activity, and reproduced the spiking patterns across brain states:\n\n\n\n\n\n\nWe found that increasing the strength of these inhibitory signals in the model decreased the fluctuations in electrical activity across entire areas of the brain. Further analysis of the experimental data supported the model‚Äôs predictions by showing that inhibitory neurons ‚Äì which act to reduce electrical activity in other neurons ‚Äì were more active when there were fewer fluctuations in activity across the brain, in both anesthetized and awake animals.\n\n\nIn particular, in awake mice, we observed decreases in noise correlations and increases in inhibitory activity during locomotion:\n\n\n\n\n\nCheck out the paper for more details.\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/unsupervised.html",
    "href": "research/posts/unsupervised.html",
    "title": "Unsupervised pretraining in biological neural networkssss",
    "section": "",
    "text": "Abstract\n\n\n\nRepresentation learning in neural networks may be implemented with supervised or unsupervised algorithms, distinguished by the availability of instruction. In the sensory cortex, perceptual learning drives neural plasticity, but it is not known whether this is due to supervised or unsupervised learning. Here we recorded populations of up to 90,000 neurons simultaneously from the primary visual cortex (V1) and higher visual areas (HVAs) while mice learned multiple tasks, as well as during unrewarded exposure to the same stimuli. Similar to previous studies, we found that neural changes in task mice were correlated with their behavioural learning. However, the neural changes were mostly replicated in mice with unrewarded exposure, suggesting that the changes were in fact due to unsupervised learning. The neural plasticity was highest in the medial HVAs and obeyed visual, rather than spatial, learning rules. In task mice only, we found a ramping reward-prediction signal in anterior HVAs, potentially involved in supervised learning. Our neural results predict that unsupervised learning may accelerate subsequent task learning, a prediction that we validated with behavioural experiments.\n\n\npaper | news article | dataset | code | preprint |\n\nThread by Lin Zhong:\n\nSimple question: How do we learn? Answer: From teachers (supervised).\n\nSure! But we also learn a lot on our own (unsupervised), and so do our mice.\n\n\n\n\n\nWe developed a virtual reality (VR) task in which mice discriminated textures in order to get reward (supervised cohort) OR they just ran for FUN (unsupervised cohort).\n\n\n\nMice learned to discriminate textures by licking in the corridor with reward.\n\n\n\n\nWe recorded up to 90,000 neurons from the visual cortex during learning to try to understand the neural mechanism. Neural activities are visualized using our sorting algorithm Rastermap with behavioral annotations.\n\n\n\n\nWe found that the plasticity in medial visual areas was mediated by unsupervised learning.\n\n\n\nMice correctly generalized the reward rule to new stimuli based on the visual similarities, behaviorally and neurally.\n\n\n\n\nMice learned to discriminate two very similar textures (leaf1 vs leaf2) by orthogonalizing them in the neural space\n\n\n\n\nLearning that only leaf1 was rewarded results in de-orthogonalization of another new leaf (leaf3).\n\n\n\n\nQuestion: Wait! We don‚Äôt need supervised learning at all? I will say no to my supervisor if that is true üôÉ.\nOur results suggest I should think twice before doing that: inside the anterior visual areas we found a representation only in the supervised learning task, which can predict the reward and was highly correlated with behavior.\n\n\n\n\nQuestion: What does unsupervised learning do? One possible answer is to pre-train our neural network for subsequent tasks. Indeed, we show that mice learned much faster after experiencing unsupervised pretraining!\n\n\n\n\nWhat is more, we found that V1 and lateral visual areas can encode novelty when seeing a new stimulus after learning. The novelty responses went away after mice got familiar with the new stimulus.\n\n\n\n\nOur results show:\n\n\nMost learning is through unsupervised learning, mediated by medial visual areas\nSupervised learning may require anterior visual areas\nA third stream (V1 + lateral) encodes novelty in both supervised and unsupervised learning\n\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/notso.html",
    "href": "research/posts/notso.html",
    "title": "Not so spontaneous: Multi-dimensional representations of behaviors and context in sensory areas",
    "section": "",
    "text": "Sensory areas are spontaneously active in the absence of sensory stimuli. This spontaneous activity has long been studied; however, its functional role remains largely unknown. Recent advances in technology, allowing large-scale neural recordings in the awake and behaving animal, have transformed our understanding of spontaneous activity. Studies using these recordings have discovered high-dimensional spontaneous activity patterns, correlation between spontaneous activity and behavior, and dissimilarity between spontaneous and sensory-driven activity patterns. These findings are supported by evidence from developing animals, where a transition toward these characteristics is observed as the circuit matures, as well as by evidence from mature animals across species. These newly revealed characteristics call for the formulation of a new role for spontaneous activity in neural sensory computation.\n\npaper\n\n\n\n\nAcross development, dimensionality of spontaneous activity increases across development, becomes more related to neural activity, and orthogonal to stimulus-driven activity.\n\n\n\n\n\n\nBehavioral modulation and orthogonality of spontaneous and evoked patterns.\n\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/suite2p.html",
    "href": "research/posts/suite2p.html",
    "title": "Suite2p: beyond 10,000 neurons with standard two-photon microscopy",
    "section": "",
    "text": "Two-photon microscopy of calcium-dependent sensors has enabled unprecedented recordings from vast populations of neurons. While the sensors and microscopes have matured over several generations of development, computational methods to process the resulting movies remain inefficient and can give results that are hard to interpret. Here we introduce Suite2p: a fast, accurate and complete pipeline that registers raw movies, detects active cells, extracts their calcium traces and infers their spike times. Suite2p runs on standard workstations, operates faster than real time, and recovers ~2 times more cells than the previous state-of-the-art method. Its low computational load allows routine detection of ~10,000 cells simultaneously with standard two-photon resonant-scanning microscopes. Recordings at this scale promise to reveal the fine structure of activity in large populations of neurons or large populations of subcellular structures such as synaptic boutons.\n\ntalk | code\n\n\n\n\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/highdim.html",
    "href": "research/posts/highdim.html",
    "title": "High-dimensional geometry of population responses in visual cortex",
    "section": "",
    "text": "Abstract\n\n\n\nA neuronal population encodes information most efficiently when its stimulus responses are high-dimensional and uncorrelated, and most robustly when they are lower-dimensional and correlated. Here we analysed the dimensionality of the encoding of natural images by large populations of neurons in the visual cortex of awake mice. The evoked population activity was high-dimensional, and correlations obeyed an unexpected power law: the nth principal component variance scaled as 1/n.¬†This scaling was not inherited from the power law spectrum of natural images, because it persisted after stimulus whitening. We proved mathematically that if the variance spectrum was to decay more slowly then the population code could not be smooth, allowing small changes in input to dominate population activity. The theory also predicts larger power-law exponents for lower-dimensional stimulus ensembles, which we validated experimentally. These results suggest that coding smoothness may represent a fundamental constraint that determines correlations in neural population codes.\n\n\npaper | talk | Quanta news article | Simons news article | preprint | data | code | original tweeprint\n\n\n\nA picture is worth a thousand words, and your brain needs billions of neurons to process it. Why do we need so many neurons? To find out, we recorded thousands of them in mouse visual cortex.\n\n\n\n\n\nOne reason to have so many neurons may be that they each have different jobs:\n\nNeuron A recognizes the pointedness of a fox‚Äôs ears, Neuron B recognizes the color of the fox‚Äôs fur.  Neuron C recognizes a fox nose,  etc\n\n\n\n\nWhen enough of these neurons activate, the brain as a whole can recognize a fox.\n\n\n\n\nWhat if some neurons ‚Äúfall asleep‚Äù on the job and don‚Äôt respond to the image? This actually happens very often, and yet the brain is remarkably robust to these failures.\n\nEven if 90% of the neurons don‚Äôt do their job, we can still recognize the fox. Even if we randomly change 90% of the pixels, we can still recognize the fox. The brain is robust to a lot of manipulations like that.\n\n\n\n\nArtificial neural networks also use millions of neurons to recognize images.\n\n\n\n\nUnlike brains, machines are not so robust to small aberrations. Here is our fox and next to it the same fox very slightly modified and now the machine thinks it‚Äôs a puffer fish!\n\n\n\n\nThese are called ‚Äúadversarial images‚Äù, because we devised them to fool the machine. How does the brain protect against these perturbations and others?\nOne protection could be to make many slightly different copies of the neurons that represent foxes. Even if some neurons fall asleep on the job, their copies might still activate.\n\nHowever, if the brain used so many neurons for every single image, we would quickly run out of neurons!\n\nThis results in an evolutionary pressure: it‚Äôs good to have many neurons do very different jobs so we can recognize lots of objects in images, but it‚Äôs also good if they share some responsibilities, so they can pick up the slack when necessary.\n\nWe found evidence for this by investigating the main dimensions of variation in the responses of 10,000 neurons. Below, each column is one neuron‚Äôs responses to several of our images.\n\n\n\n\nThe largest two dimensions were distributed broadly across all neurons, as you see below. Any neuron could contribute to these and pick up the slack if the other neurons did not respond.\n\n\n\n\nThe next 8 dimensions each were smaller and distributed more sparsely across neurons. If a neuron was asleep, it was still likely a few others could represent these dimensions in its place.\n\n\n\n\nThe next 30 dimensions revealed ever more intricate structure‚Ä¶\n\n\n\n\nAnd so did the next 160 dimensions‚Ä¶\n\n\n\n\nAnd so on, this kept on going, with the N-th dimension being about N times smaller than the biggest dimension.\n\nThis distribution of activity is called a ‚Äúpower-law‚Äù.\n\n\n\n\nHowever, this was not just any power-law, it had a special exponent of approx 1. We did some math and showed that a power-law with this exponent must be borderline fractal.\n\nA fractal is a mathematical object that has structure at many different spatial scales, like the Mandelbrot set below:\n\n\n\n\n\nThis Inceptionism movie is also a kind of fractal:\n\n\n\n\n\nThe neural activity was so close to being a fractal, and just barely avoided it because it‚Äôs exponent was 1.04, not 1 or smaller.\n\nAn exponent of 1.04 is the sweet spot: as high-dimensional as possible without being a fractal.\n\nNot being a fractal allows neural responses to be continuous and smooth, which are the minimal protections neurons need so that we don‚Äôt confuse a fox with a puffer fish!\nWe shared the data, and the code to run the analyses. End of story, for now. data: (figshare) code: (github.com/MouseLand/stringer-pachitariu-et-al-2018b)\n\n\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/cellpose2.html",
    "href": "research/posts/cellpose2.html",
    "title": "Cellpose 2.0: how to train your own model",
    "section": "",
    "text": "Abstract\n\n\n\nPretrained neural network models for biological segmentation can provide good out-of-the-box results for many image types. However, such models do not allow users to adapt the segmentation style to their specific needs and can perform suboptimally for test images that are very different from the training images. Here we introduce Cellpose 2.0, a new package that includes an ensemble of diverse pretrained models as well as a human-in-the-loop pipeline for rapid prototyping of new custom models. We show that models pretrained on the Cellpose dataset can be fine-tuned with only 500‚Äì1,000 user-annotated regions of interest (ROI) to perform nearly as well as models trained on entire datasets with up to 200,000 ROI. A human-in-the-loop approach further reduced the required user annotation to 100‚Äì200 ROI, while maintaining high-quality segmentations. We provide software tools such as an annotation graphical user interface, a model zoo and a human-in-the-loop pipeline to facilitate the adoption of Cellpose 2.0.\n\n\npaper | talk | tutorial | news & views | preprint | code | original tweeprint\n\nThis is an upgrade to Cellpose; if you‚Äôre unfamiliar with Cellpose, check it out here.\nCellpose 2.0 thread:\n\nYou can now train your own state-of-the-art models in less than 1 hour, all from the GUI. Massive improvements for some images!\n\n\n\n\n\n\nCellpose 1.0 is great, but we were getting reports of imperfect segmentations on some image categories, e.g.¬†on some new large-scale datasets, like TissueNet and LiveCell (see the two images above).\nAs we dove into these datasets, we realized that different people just segment cells in different ways. Here are some representative examples from different datasets:\n\n\n\n\n\nNotice how in different images more or less of the cytoplasm is segmented? Or how nuclei may or may not be segmented when they don‚Äôt have cytoplasm? Or how too dense regions are sometimes not annotated? The list goes on and on.\n\nIt‚Äôs impossible for a single model like Cellpose 1.0 to segment the same image in multiple ways. For this we had to build multiple models with different segmentation styles, i.e.:\n\n\n\n\n\nAll these segmentation styles are available in Cellpose 2.0 at the click of a button. Try them out!\n\n\n\n\n\nThis led us to think more carefully about personalized models for everyone. The main challenge is that deep learning typically requires a lot of training data‚Ä¶\nExcept it doesn‚Äôt. Not necessarily. When we initialize with Cellpose 1.0, a new model can be trained with ~500 segmented ROIs. The gains beyond that are minimal.\n\n\n\n\n\nAnd we can further reduce the training data requirement to 100-200 ROIs simply with a human-in-the-loop approach, where the user fixes the mistake of the algorithm instead of segmenting from scratch.\n\n\n\n\n\nAs the user segments more, new models are trained that require even fewer corrections. Within a few iterations, very competitive models can be trained, which match human performance (see blue curve below):\n\n\n\n\n\nHere‚Äôs our human-in-the-loop pipeline in action. We start by correcting the mistakes of Cellpose 1.0 (2x speedup)\n\n\n\n\n\nThen we train a new model on the single image we just segmented:\n\n\n\n\n\nAfter ~30 minutes, the user trains the fifth model, and this provides great results on various new images:\n\n\n\n\n\nSee the entire 30 minute procedure on youtube.\nFinal reveal: I [Marius] was the ‚Äúhuman-in-the-loop‚Äù for all experiments! It‚Äôs actually quite fun. Try it out and please send us more training data via upload in the GUI. code: https://github.com/MouseLand/cellpose\n\n\nThe End.\n\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/posts/critical_init.html",
    "href": "research/posts/critical_init.html",
    "title": "A critical initialization for biological neural networks",
    "section": "",
    "text": "Abstract\n\n\n\nArtificial neural networks learn faster if they are initialized well. Good initializations can generate high-dimensional macroscopic dynamics with long timescales. It is not known if biological neural networks have similar properties. Here we show that the eigenvalue spectrum and dynamical properties of large-scale neural recordings in mice (two-photon and electrophysiology) are similar to those produced by linear dynamics governed by a random symmetric matrix that is critically normalized. An exception was hippocampal area CA1: population activity in this area resembled an efficient, uncorrelated neural code, which may be optimized for information storage capacity. Global emergent activity modes persisted in simulations with sparse, clustered or spatial connectivity. We hypothesize that the spontaneous neural activity reflects a critical initialization of whole-brain neural circuits that is optimized for learning time-dependent tasks.\n\n\npreprint | code | bluesky thread\n\nThread:\n\nWhat if‚Ä¶ spontaneous neural activity üß† reflects the baseline rumblings of a brainwide dynamical system initialized for learning? We find that the rumblings have macroscopic properties like those emerging from linear symmetric, critical systems üßµ #neuroscience #neuroAI\n\n\n\n\n\n\nLong timescales and large principal components (PCs) can be produced by a dynamical system with random connectivity and independent stochastic inputs, if the connectivity matrix is critically-normalized.\n\n\n\n\n\n\nFurthermore, the principal components in the model decay as a power-law, a phenomenon we have previously reported in large-scale neural recordings: https://www.science.org/doi/10.1126/science.aav7893; https://www.nature.com/articles/s41586-019-1346-5\nIn V1 and brainwide ephys recordings we observed that the PC variances decayed as a power-law with exponents of 0.7-0.85, consistent with symmetric, critically-normalized simulations.\n\n\n\n\n\n\nBut in hippocampus, we observed exponents around 0.5 that did not change after shuffling, suggesting that hippocampal activity is closer to completely independent neurons.\n\n\n\n\n\n\nThe model also predicted that higher PCs have longer timescales, which was true in the data.\n\n\n\n\n\n\nAn estimate of the dynamics matrix of the data (using DMD) revealed mostly real eigenvalues, further supporting symmetric dynamics.\n\n\n\n\n\n\nGlobal emergent activity modes persisted in simulations with sparse, clustered or spatial connectivity.\n\n\n\n\n\n\nSimulations with spatial connectivity replicated several of the properties we observed in the neural recordings, such as a spatial dependence of top correlated neuron pairs, and top PCs which were globally spread across cortex.\n\n\n\n\n\n\nWe hypothesize that the spontaneous neural activity reflects a critical initialization of whole-brain neural circuits that is optimized for learning tasks that are time-dependent and working-memory dependent. More details in the paper by @marius10p.bsky.social.\n\n\n\n\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  },
  {
    "objectID": "research/index._old.html",
    "href": "research/index._old.html",
    "title": "Research",
    "section": "",
    "text": "A simplified minimodel of visual cortical neurons\n\n\n\nVision\n\nThousands of neurons\n\nMachine learning\n\n\n\nSimplified and interpretable ‚Äúminimodels‚Äù are sufficient to explain complex visual responses in mouse and monkey V1.\n\n\n\n\n\nFengtong Du, Miguel Angel N√∫√±ez-Ochoa, Marius Pachitariu‚Ä†, Carsen Stringer‚Ä†\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised pretraining in biological neural networkssss\n\n\n\nVision\n\nThousands of neurons\n\nBehavior\n\n\n\nUnsupervised learning - exposure to stimuli without rewards - drives large changes in neural activity in visual cortex, particularly in higher order medial visual areas.\n\n\n\n\n\nLin Zhong, Scott Baptista, Rachel Gattoni, Jon Arnold, Daniel Flickinger, Carsen Stringer‚Ä†, Marius Pachitariu‚Ä†\n\n\n\n\n\n\n\n\n\n\n\n\nCellpose3: one-click image restoration for improved cellular segmentation\n\n\n\nSegmentation\n\nMachine learning\n\nTools\n\n\n\nSoftware for image restoration that works across many cell types and imaging modalities.\n\n\n\n\n\nCarsen Stringer, Marius Pachitariu\n\n\n\n\n\n\n\n\n\n\n\n\nA critical initialization for biological neural networks\n\n\n\nThousands of neurons\n\nNeural modeling\n\n\n\nNeural recordings resemble linear dynamical systems governed by a symmetric connectivity matrix.\n\n\n\n\n\nMarius Pachitariu, Lin Zhong, Alexa Gracias, Amanda Minisi, Crystall Lopez, Carsen Stringer\n\n\n\n\n\n\n\n\n\n\n\n\nRastermap: A Discovery Method for Neural Population Recordings\n\n\n\nThousands of neurons\n\nMachine learning\n\nTools\n\n\n\nAnalysis tool for large-scale neural data which allows users to explore dynamical and spatial relationships among neurons.\n\n\n\n\n\nCarsen Stringer, Lin Zhong, Atika Syeda, Fengtong Du, Maria Kesa, Marius Pachitariu\n\n\n\n\n\n\n\n\n\n\n\n\nSpike sorting with Kilosort4\n\n\n\nMachine learning\n\nTools\n\n\n\nAnalysis tool for accurate spike sorting, which is the computational process of extracting the firing times of single neurons from recordings of local electrical fields.\n\n\n\n\n\nMarius Pachitariu, Shaswat Sridhar, Jacob Pennington, Carsen Stringer\n\n\n\n\n\n\n\n\n\n\n\n\nFacemap: a framework for modeling neural activity based on orofacial tracking\n\n\n\nBehavior\n\nThousands of neurons\n\nMachine learning\n\nTools\n\n\n\nAnalysis tool for tracking mouse face keypoints and relating them to large-scale neural activity, using convolutional neural networks.\n\n\n\n\n\nAtika Syeda, Lin Zhong, Renee Tung, Will Long, Marius Pachitariu‚Ä†, Carsen Stringer‚Ä†\n\n\n\n\n\n\n\n\n\n\n\n\nCellpose 2.0: how to train your own model\n\n\n\nSegmentation\n\nMachine learning\n\nTools\n\n\n\nSoftware for users to quickly and easily create accurate segmentation models for their own data.\n\n\n\n\n\nMarius Pachitariu, Carsen Stringer\n\n\n\n\n\n\n\n\n\n\n\n\nNot so spontaneous: Multi-dimensional representations of behaviors and context in sensory areas\n\n\n\nThousands of neurons\n\nBehavior\n\n\n\nSpontaneous activity across species is related to motor movements. These motor-driven patterns emerge during development, diverging from sensory-driven activity.\n\n\n\n\n\nLilach Avitan, Carsen Stringer\n\n\n\n\n\n\n\n\n\n\n\n\nHigh precision coding in visual cortex\n\n\n\nVision\n\nThousands of neurons\n\n\n\nVisual cortex encodes stimuli highly precisely, far surpassing behavioral precision in mice and humans. In a task, visual cortex does not contribute to behavioral variability.\n\n\n\n\n\nCarsen Stringer, Michalis Michaelos, Dmitri Tsyboulski, Sarah E. Lindo, Marius Pachitariu\n\n\n\n\n\n\n\n\n\n\n\n\nCellpose: a generalist algorithm for cellular segmentation\n\n\n\nSegmentation\n\nMachine learning\n\nTools\n\n\n\nAccurate deep learning-based cellular segmentation tool that works for a wide variety of images, and includes an easy to use GUI.\n\n\n\n\n\nCarsen Stringer, Tim Wang, Michalis Michaelos, Marius Pachitariu\n\n\n\n\n\n\n\n\n\n\n\n\nHigh-dimensional geometry of population responses in visual cortex\n\n\n\nVision\n\nThousands of neurons\n\n\n\nNeural population activity in response to natural images is high-dimensional, and the activity correlations obey a power law of 1/n.\n\n\n\n\n\nCarsen Stringer*, Marius Pachitariu*, Nicholas Steinmetz, Matteo Carandini‚Ä†, Kenneth D Harris‚Ä†\n\n\n\n\n\n\n\n\n\n\n\n\nSpontaneous behaviors drive multidimensional, brainwide activity\n\n\n\nThousands of neurons\n\nBehavior\n\nMachine learning\n\n\n\nOngoing neural activity is high-dimensional and driven by the mouse‚Äôs behavior. These behavioral representations are orthogonal to stimulus-driven responses.\n\n\n\n\n\nCarsen Stringer*, Marius Pachitariu*, Nicholas Steinmetz, Charu Bai Reddy, Matteo Carandini‚Ä†, Kenneth D Harris‚Ä†\n\n\n\n\n\n\n\n\n\n\n\n\nSuite2p: beyond 10,000 neurons with standard two-photon microscopy\n\n\n\nThousands of neurons\n\nSegmentation\n\nMachine learning\n\nTools\n\n\n\nSuite2p enables fast and accurate processing of large-scale neural recordings.\n\n\n\n\n\nMarius Pachitariu, Carsen Stringer, Sylvia Schr√∂der, Mario Dipoppa, L. Federico Rossi, Matteo Carandini, Kenneth D. Harris\n\n\n\n\n\n\n\n\n\n\n\n\nInhibitory control of correlated intrinsic variability in cortical networks\n\n\n\nNeural modeling\n\nBehavior\n\n\n\nCortical networks exhibit large-scale fluctuations which creates noise correlations that impact sensory coding. Modeling and data analysis show that inhibition can control these fluctuations.\n\n\n\n\n\nCarsen Stringer*, Marius Pachitariu*, Nicholas Steinmetz, Michael Okun, Peter Bartho, Kenneth D Harris, Maneesh Sahani, Nicholas Lesica\n\n\n\n\n\nNo matching items\n\n  Powered by Quarto. ¬© Wong-Campos lab, 2025."
  }
]